{"cells":[{"cell_type":"markdown","metadata":{"id":"DUhY7UPB-xA4"},"source":["**Install packages:** Install the necessary packages for running the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bypXPAcf-xA6","outputId":"7e076fd3-1f07-4942-efa0-385a1f963a89"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /private/var/folders/0c/j4c53d0102lcr2b5rk247yrm0000gn/T/pip-req-build-zrd2r15y\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /private/var/folders/0c/j4c53d0102lcr2b5rk247yrm0000gn/T/pip-req-build-zrd2r15y\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numba in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (0.59.0)\n","Requirement already satisfied: numpy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (1.26.4)\n","Requirement already satisfied: torch in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (2.2.1)\n","Requirement already satisfied: tqdm in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (4.65.0)\n","Requirement already satisfied: more-itertools in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (10.1.0)\n","Requirement already satisfied: tiktoken in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (0.6.0)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from numba->openai-whisper==20231117) (0.42.0)\n","Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from tiktoken->openai-whisper==20231117) (2023.10.3)\n","Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (4.9.0)\n","Requirement already satisfied: sympy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (1.12)\n","Requirement already satisfied: networkx in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (2023.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":true,"id":"MFQWN7eV-xA8","outputId":"52694f81-033e-4e79-a6cb-b7c05b2323cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716212095802,"user_tz":-240,"elapsed":15953,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2Oc5ZiTq-xA9","outputId":"e9b662d8-e51b-49a0-fb6e-cf9673820220"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (2.2.1)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (2023.10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WpOQqZq--xA9","outputId":"aff28eb0-ac70-4583-cf70-db267f8bf280"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (2.18.0)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.10.0)\n","Requirement already satisfied: aiohttp in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.21.4)\n","Requirement already satisfied: packaging in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install datasets"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":true,"id":"30qny2-G-xA-","outputId":"e3582265-4ccb-47c6-cb72-abb41382b215","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716212066213,"user_tz":-240,"elapsed":20486,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"]}],"source":["pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4UFQnl8V-xA_","outputId":"3c09411a-b1d8-452c-b8d5-42572dbc1a54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: librosa in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (0.10.1)\n","Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.26.4)\n","Requirement already satisfied: scipy>=1.2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.2.0)\n","Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.59.0)\n","Requirement already satisfied: soundfile>=0.12.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.8.1)\n","Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (4.9.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.0.3)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pooch>=1.0->librosa) (3.10.0)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pooch>=1.0->librosa) (23.1)\n","Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pooch>=1.0->librosa) (2.31.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n","Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: pycparser in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install librosa"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":true,"id":"ppVTiVHi-xA_","outputId":"b4bd9de4-4409-48aa-a877-3a2377c3618e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716212079853,"user_tz":-240,"elapsed":13645,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.4)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n","Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.9.1)\n"]}],"source":["pip install jiwer"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"t3dJCYl6-xBA","outputId":"806ac284-d74d-466b-fd17-43de656df65d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: pyyaml in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (2.2.1)\n","Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (0.21.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n","Requirement already satisfied: requests in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install accelerate -U"]},{"cell_type":"code","source":["pip install yt_dlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xOSXJaQ3-x-Y","executionInfo":{"status":"ok","timestamp":1716040485348,"user_tz":-240,"elapsed":34282,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}},"outputId":"834a174d-8fd5-4422-d785-c40ec2cd2da7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting yt_dlp\n","  Downloading yt_dlp-2024.4.9-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting brotli (from yt_dlp)\n","  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2024.2.2)\n","Collecting mutagen (from yt_dlp)\n","  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycryptodomex (from yt_dlp)\n","  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.31.0)\n","Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.0.7)\n","Collecting websockets>=12.0 (from yt_dlp)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt_dlp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt_dlp) (3.7)\n","Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt_dlp\n","Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt_dlp-2024.4.9\n"]}]},{"cell_type":"markdown","metadata":{"id":"uT4Kpqga-xBB"},"source":["**Setting up PyTorch Device:** Check if CUDA is available and set the device to GPU if possible; otherwise, fall back to CPU."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"Gl89I9N0-xBB","outputId":"455cdd5d-864f-4658-a6cf-5c33b490dd87","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716208564812,"user_tz":-240,"elapsed":3,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"7UxWaB1f-xBB"},"source":["**Dataset Preparation:** Import necessary libraries and define a function to create a dataset from a TSV file, which reads the file, adjusts the file paths, and converts the data into a Hugging Face dataset format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaaH_ezG-xBB"},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","import pandas as pd\n","from pathlib import Path\n","\n","base_path = Path('cv-corpus-17.0-2024-03-15/hy-AM')\n","\n","def create_dataset_from_dataframe(split_name, base_path):\n","    df = pd.read_csv(base_path / f'{split_name}.tsv', sep='\\t')\n","    clips_path = base_path / 'clips'\n","    df['path'] = df['path'].apply(lambda x: str(clips_path / x))\n","    dataset = Dataset.from_pandas(df)\n","    return dataset\n","\n","my_common_voice = DatasetDict()\n","\n","my_common_voice['train'] = create_dataset_from_dataframe('train', base_path)\n","my_common_voice['test'] = create_dataset_from_dataframe('test', base_path)\n","my_common_voice['validation'] = create_dataset_from_dataframe('dev', base_path)"]},{"cell_type":"markdown","metadata":{"id":"uWl95-Tu-xBC"},"source":["**Clean Dataset:** Remove unnecessary columns like 'client_id', 'up_votes', etc., from the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSTEaiX_-xBC"},"outputs":[],"source":["my_common_voice = my_common_voice.remove_columns([\"client_id\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accents\", \"variant\", \"locale\", \"segment\"])"]},{"cell_type":"markdown","metadata":{"id":"FqlnKuNt-xBC"},"source":["**Print Dataset Overview:** Display the structure of the dataset to confirm the changes and view the remaining columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lc4O6VqQ-xBC","outputId":"9e6e55ed-b526-491d-8835-b89fdf82cc71"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['path', 'sentence_id', 'sentence', 'sentence_domain'],\n","        num_rows: 6180\n","    })\n","    test: Dataset({\n","        features: ['path', 'sentence_id', 'sentence', 'sentence_domain'],\n","        num_rows: 4281\n","    })\n","    validation: Dataset({\n","        features: ['path', 'sentence_id', 'sentence', 'sentence_domain'],\n","        num_rows: 4214\n","    })\n","})\n"]}],"source":["print(my_common_voice)"]},{"cell_type":"markdown","metadata":{"id":"8lLoXPMQ-xBD"},"source":["**Load Feature Extractor and Tokenizer:** Load the Whisper feature extractor and tokenizer from Hugging Face's Transformers library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b__6uGN7-xBD"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QedlbPk-xBD","outputId":"fc558db4-c95d-4b5d-910a-3e65bd41259b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")"]},{"cell_type":"markdown","metadata":{"id":"C3-Al7bD-xBD"},"source":["**Transcription Tokenization and Decoding:** Perform tokenization on a sample text, and decode it to ensure the process retains the original text without any special tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lLbZ2EK-xBD","outputId":"70b8fe10-a545-471c-cab5-f95fd72b0579"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input:                 Հորիզոնին երևում են մերկ ծառեր և տներ։\n","Decoded w/ special:    <|startoftranscript|><|hy|><|transcribe|><|notimestamps|>Հորիզոնին երևում են մերկ ծառեր և տներ։<|endoftext|>\n","Decoded w/out special: Հորիզոնին երևում են մերկ ծառեր և տներ։\n","Are equal:             True\n"]}],"source":["input_str = my_common_voice[\"train\"][0][\"sentence\"]\n","labels = tokenizer(input_str).input_ids\n","decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n","decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n","\n","print(f\"Input:                 {input_str}\")\n","print(f\"Decoded w/ special:    {decoded_with_special}\")\n","print(f\"Decoded w/out special: {decoded_str}\")\n","print(f\"Are equal:             {input_str == decoded_str}\")"]},{"cell_type":"markdown","metadata":{"id":"ScdZxlvq-xBD"},"source":["**Print a Sample Entry:** Display a specific example from the training set to verify the data structure and contents."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"RSH8GYqX-xBE","outputId":"dc719a77-711b-47de-b8b4-535559594598"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'path': 'cv-corpus-17.0-2024-03-15/hy-AM/clips/common_voice_hy-AM_26466425.mp3', 'sentence_id': '014e1b24fd333a70a23e79c20cd4822a843711386f585a6a61f2bbbd6b796f7a', 'sentence': 'Հորիզոնին երևում են մերկ ծառեր և տներ։', 'sentence_domain': None}\n"]}],"source":["print(my_common_voice[\"train\"][0])"]},{"cell_type":"markdown","metadata":{"id":"FwSDNiPc-xBE"},"source":["**Define Data Preparation Function:** Create a function to process the audio data: loading, resampling, extracting features using Whisper, and encoding the labels for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1v4qx4T-xBE"},"outputs":[],"source":["import librosa\n","\n","def load_and_resample_audio(file_path, target_sr=16000):\n","    audio, _ = librosa.load(file_path, sr=target_sr)\n","    return {'array': audio, 'sampling_rate': target_sr}"]},{"cell_type":"markdown","metadata":{"id":"lDCXD-t5-xBE"},"source":["**Apply Data Preparation:** Use the 'map' method to apply the data preparation function across the dataset, ensuring the data is ready for input into the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyfqRO6t-xBE"},"outputs":[],"source":["def prepare_dataset(batch):\n","    path = batch[\"path\"]\n","    audio = load_and_resample_audio(path)\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"referenced_widgets":["c5c50987682c4eddbc5a07ed5b9c65b5","08316da5891944e4b8b7fd2601683630","308c432988e943a0a8b88f386eddb2cd"]},"id":"lcUR-h3I-xBE","outputId":"5e252aef-b0c2-4eb5-a505-8674eeed1d59"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5c50987682c4eddbc5a07ed5b9c65b5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08316da5891944e4b8b7fd2601683630","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"308c432988e943a0a8b88f386eddb2cd","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["my_common_voice = my_common_voice.map(prepare_dataset, remove_columns=my_common_voice.column_names[\"train\"])"]},{"cell_type":"markdown","metadata":{"id":"RVZeQk3j-xBE"},"source":["**Define Data Collator Class:** This class is designed to handle batching of speech sequences which might have variable lengths. It processes the input features and labels, padding them appropriately for the model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSe9VKwz-xBE"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","        return batch"]},{"cell_type":"markdown","metadata":{"id":"xnDGb9Q8-xBF"},"source":["**Initialize Processor and Data Collator:** Load the Whisper processor and instantiate the custom data collator class using this processor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kD4jOKV8-xBF","outputId":"9d0fdc98-fa90-42c0-c66f-d2e47f74558c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XXoE7Ud-xBF"},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","metadata":{"id":"44JJJbx8-xBF"},"source":["**Setup Evaluation Metrics:** Import evaluation metrics specifically for word error rate (WER) and character error rate (CER) to be used during model testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6f0zwVD-xBF"},"outputs":[],"source":["import evaluate\n","\n","metric_wer = evaluate.load(\"wer\")\n","metric_cer = evaluate.load(\"cer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33_-VhXC-xBF"},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)\n","    cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer, \"cer\": cer}"]},{"cell_type":"markdown","metadata":{"id":"zor-ESi--xBF"},"source":["**Load Pretrained Model for Conditional Generation:** Import the WhisperForConditionalGeneration class from the transformers library and load a pretrained Whisper model configured for generating text in Armenian. The model configuration is also adjusted to set the generation language to Armenian."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlS2cgUM-xBG"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n","model.generation_config.language = \"armenian\""]},{"cell_type":"markdown","metadata":{"id":"MrrJzg8Q-xBG"},"source":["**Configure Model Decoding:** Configure the model's decoding behavior by setting forced_decoder_ids to None and clearing any suppress_tokens. These settings adjust how the model generates output, ensuring that it does not force any specific decoder token IDs and does not suppress any tokens during the decoding phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LibVQDGi-xBG"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"markdown","metadata":{"id":"5rYycKix-xBG"},"source":["**Create Logging Directory** Checks if a logging directory exists and if not, it creates one. This directory will be used to store training logs, which are crucial for monitoring the training process and evaluating model performance over time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"151EfRMz-xBG"},"outputs":[],"source":["import os\n","\n","path = \"logs/small_25_epoch_48_hours\"\n","if not os.path.exists(path):\n","    os.makedirs(path)\n","    logdir = path\n","else:\n","    logdir = path"]},{"cell_type":"markdown","metadata":{"id":"3y7ChSZ4-xBG"},"source":["**Define Training Arguments:** Setup various training parameters such as batch sizes, learning rates, and evaluation strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzdeUeKx-xBG"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"whisper-small-hy-48-hours\",\n","    per_device_train_batch_size=16,\n","    logging_dir=logdir,\n","    gradient_accumulation_steps=1,\n","    learning_rate=1e-5,\n","    warmup_steps=1,\n","    num_train_epochs=1,\n","    gradient_checkpointing=True,\n","    fp16=False,\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_strategy = \"epoch\",\n","    evaluation_strategy = \"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"yZpCAlJA-xBG"},"source":["**Prepare Model for Training:** Move the model to the designated computing device (GPU or CPU)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"-o-pyAXR-xBH","outputId":"62e717ac-fafc-4b64-a6e2-cbc67fc18944"},"outputs":[{"data":{"text/plain":["WhisperForConditionalGeneration(\n","  (model): WhisperModel(\n","    (encoder): WhisperEncoder(\n","      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n","      (embed_positions): Embedding(1500, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperEncoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): WhisperDecoder(\n","      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n","      (embed_positions): WhisperPositionalEmbedding(448, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperDecoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"cysKJ_O--xBH"},"source":["**Initialize Training Setup:** Combine all necessary components like the model, datasets, data collator, and metrics function into the Seq2SeqTrainer for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tPhFFr4-xBH"},"outputs":[],"source":["from transformers import TrainerCallback\n","\n","class SaveLastModelCallback(TrainerCallback):\n","    def on_train_end(self, args, state, control, **kwargs):\n","        trainer.save_model(output_dir=args.output_dir + \"/last_epoch\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1REAfQzc-xBH"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=my_common_voice[\"train\"],\n","    eval_dataset=my_common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n","    callbacks=[SaveLastModelCallback()],\n",")"]},{"cell_type":"markdown","metadata":{"id":"GzKDIUuP-xBH"},"source":["**Training:** Execute the training process using the trainer configuration established in previous cells."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"yBDHnxZI-xBH","outputId":"2d5aaad9-b79a-485e-b106-b7581358ddf3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/user1801/anaconda3/envs/speech/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/1 : < :, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"ZO4frfYp-xBH"},"source":["**Load Pretrained Model:** Load a Whisper model pre-trained for conditional generation with specific configurations set for Armenian language."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfLAvUpb-xBH","outputId":"b69aefa0-a108-4011-c502-a5a4221b7401"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["WhisperForConditionalGeneration(\n","  (model): WhisperModel(\n","    (encoder): WhisperEncoder(\n","      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n","      (embed_positions): Embedding(1500, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperEncoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): WhisperDecoder(\n","      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n","      (embed_positions): WhisperPositionalEmbedding(448, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperDecoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor\n","import evaluate\n","import librosa\n","\n","metric_wer = evaluate.load(\"wer\")\n","metric_cer = evaluate.load(\"cer\")\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"whisper-small-hy-48-hours/checkpoint-9675\")\n","model.generation_config.language = \"armenian\"\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"whisper-small-hy-48-hours/checkpoint-9675\")\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["0a27128bc11a4ec88fb4b14ad02c5470"]},"id":"7TxH-BkT-xBI","outputId":"792967dd-b3fd-4d91-9b08-4d3f2cc2ac41"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a27128bc11a4ec88fb4b14ad02c5470","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def map_to_pred(batch):\n","    path = batch[\"path\"]\n","    audio, _ = librosa.load(batch[\"path\"], sr=16000)\n","    input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n","    batch[\"reference\"] = processor.tokenizer._normalize(batch['sentence'])\n","\n","    with torch.no_grad():\n","        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n","    transcription = processor.decode(predicted_ids)\n","    batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n","    return batch\n","\n","result = my_common_voice[\"test\"].map(map_to_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQsodafY-xBI","outputId":"dcc7479a-2dc1-421c-9c37-4e5012572f9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test WER: 46.053\n","Test CER: 10.702\n"]}],"source":["print(\"Test WER: {:.3f}\".format(100 * metric_wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"])))\n","print(\"Test CER: {:.3f}\".format(100 * metric_cer.compute(references=result[\"reference\"], predictions=result[\"prediction\"])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Odxx34IX-xBI"},"outputs":[],"source":["model.eval()\n","\n","def transcribe_audio(audio_file):\n","    audio, sampling_rate = librosa.load(audio_file, sr=16000, mono=True)\n","    inputs = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=sampling_rate)\n","    inputs = inputs.to(device)\n","\n","    with torch.no_grad():\n","        predictions = model.generate(inputs.input_features)\n","\n","    text = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-IpZsex-xBI","outputId":"7b5113c8-d5ba-47c1-c887-c2d733077f91"},"outputs":[{"name":"stdout","output_type":"stream","text":["Պատմողն աշխատանք է ստանում գրադարանում։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39509489.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbvCChUB-xBI","outputId":"8711f09d-38cd-49aa-c343-4d88d37267a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Այս պատճառով, որոշ ծրագրային բագեր չեն հայտնաբերվում մինչև ծրագիրը չի տեստավորվում։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39517769.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwkE102C-xBI","outputId":"477586b2-cafa-4e31-b2bc-43a351d0e0bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Նա կոչ չէր անում սպիտակամորդների դեմ պայքարել՝ մարգարեները չէին խոսումները անունից։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39427421.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLYcOYXZ-xBI","outputId":"d6059939-3cf3-40c6-c1c1-333cfc978906"},"outputs":[{"name":"stdout","output_type":"stream","text":["Թեկլեն սկսեց զգալ ցամակ աչքերով, բայց այնպիսի աղեկտուլ խոսքերով, ուրնույնի սկոր։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39295677.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHn-MUVy-xBI","outputId":"8ca287f2-6bd4-4782-8a1f-7959e0644f1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[youtube] Extracting URL: https://www.youtube.com/watch?v=-OhScYMGqvc\n","[youtube] -OhScYMGqvc: Downloading webpage\n","[youtube] -OhScYMGqvc: Downloading ios player API JSON\n","[youtube] -OhScYMGqvc: Downloading android player API JSON\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"-OhScYMGqvc\")\n"]},{"name":"stdout","output_type":"stream","text":["[youtube] -OhScYMGqvc: Downloading m3u8 information\n","[info] -OhScYMGqvc: Downloading 1 format(s): 251\n","[download] Destination: -OhScYMGqvc.webm\n","[download] 100% of    3.10MiB in 00:00:00 at 7.28MiB/s     \n","[ExtractAudio] Destination: -OhScYMGqvc.mp3\n","Deleting original file -OhScYMGqvc.webm (pass -k to keep)\n","Արհեստական բանականթյուն, խելացի ռոբոտներ, չաց ջիփիթիտ, զգացել եք վերջերց միջքան ինքոզում արհեստական բանականթյուն մասին։ողչույն, ես ունարենը զգացելենք և որոշել ենք ստեղծել հաղորդաշար, որի ընթացքում կոգոսեանք արհեստական բանականթյունը շատերը պատկերաստում են այսպես… Պեց նավ, ամեն դեպքում ինչը արհեստական բանականությունը, մի լովը… արհեստական բանականություն, գիտություն, որով մեքենաները դառնում են խելացի…Նեսած կլինեակ, որ արհեստական բանականությունը վերասնալուէ է բալորմասնագիտությունները, կամոր մարմարկանց կայլիկ այլևս չինինելու՞՞թէէէէէէէէէէէէէէէէ� Դեը արհեստական բանականությունն այսօր ամենուր է։բժշկության մեջ բարձ վիրահատություններ անով ռոբոտներ են ստեացվում, արհեստական բանականությամբ ստեացված նկարիսեռ չարճեպիթին։ ընդամենը մեկտարի առաջ ամերիկյան օքննեիայի տեխտալոգեկան ինկերության այսգործիքը քչերին էր հայտնի՝ շատ կարծրամում մեկլեռ՝ է՝ ստեղծուհհ՝ ենըը՞ստեղծըը՞ըըըըըըըըըըըըըըըըըըըըըըըըըըըը� Մամերիկյան օգլնէիայի տեղտալոգեկան ինկերության այս գործիքը չէր իներ հայտնի։շատ կարծ ժամանակում չատճիպիթին այնպիսի հաջողություն գրանցեց, որ նրա մասին խոսում էին ամենուր՝ չատճոթյան ավելիքան ավելիքան թառիկան ավելիքան թառիկան ավելիքան թառիկան թառիկան ավելիքան թառիկան թառիկան ավելիքան թառիկան ավելիքան ավելիքան ավելիքան ավելիքան ավելիքան ավելիքան ավելիքան ավելիքան ավելիքան ավ Օնարբաթկեցումս մի չորսաղորդում է տո՝ էր մել դեմքով նսլվոմ էր սաղորդումա։ Հաու այդ պատճառով շատելն ասում են, որ գուգիլի բարգնոյինրուն մածքի կրոքը շատավելի լավն են, քանի որ իրենք ունեն տվյալ պահի իրադարձությունների մասին տեղակություններ, ու ինչա ասում է փներայետ մասին, որ ու իսչե՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէ՞ս չէս չէս չէս չէս չէս չէս չէս չէս չէս չէս չէսչէս չէսչէսչէսէսէսէսէսէսէսէս Որ ենք ունեն տվյալ պահի իրադարձությունների մասին տեղակուրծում էր, ու ինչա ասումը փնեյայի հետ մասին… օգնեյային էլ ասումը, որ աշխատելու են ավելի շատ ստիգալներ հասուների դարձներու համար… աշխատելու ենք ցուցանիշը բարելավելու ուղղությամբ… իսկ մջևներանք աշխատումային գուգլներ շապատներկայացյացյացյացյացյացյացյացյացչածծէբիթիի մոդելներին… այնձոր բրցավասկրին… Լայմթուրպումասկրին… այուսքու� - Հելո են ոյս գիպիտի հետլայն նուզ, այմ ֆուչուրը ֆոր, են աի ավատար նուզ անգր.- Գանակցիում սիքս, ու ուղտն վրստ այլ ավատար ժումալիստ, հիրար թէ՛ս թոպ հետլայությանց,- լեզելես են մույն գիպիտի մասին, որ օր եքսանչոշրան էիայե էթենիմա,- մյուսիհաաւում սմտացումամբ, արգործում են ձեռքիստանայում,- ճալ, գիլկիմացինքան՞։\n"]}],"source":["import yt_dlp\n","import os\n","\n","\n","def download_youtube_audio(url):\n","    ydl_opts = {\n","        'format': 'bestaudio/best',\n","        'postprocessors': [{\n","            'key': 'FFmpegExtractAudio',\n","            'preferredcodec': 'mp3',\n","            'preferredquality': '192',\n","        }],\n","        'outtmpl': '%(id)s.%(ext)s',\n","        'quiet': False\n","    }\n","    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n","        info_dict = ydl.extract_info(url, download=True)\n","        filename = ydl.prepare_filename(info_dict)\n","        base, ext = os.path.splitext(filename)\n","        return base + '.mp3'\n","\n","audio_file = download_youtube_audio(\"https://www.youtube.com/watch?v=-OhScYMGqvc\")\n","\n","def segment_audio(audio_file, segment_length=30, overlap=5, sample_rate=16000):\n","    audio, sr = librosa.load(audio_file, sr=sample_rate, mono=True)\n","    total_length = librosa.get_duration(y=audio, sr=sr)\n","\n","    start = 0\n","    while start + segment_length <= total_length:\n","        end = start + segment_length\n","        yield audio[int(start * sr):int(end * sr)]\n","        start += (segment_length - overlap)\n","\n","def transcribe_segments(audio_file):\n","    texts = []\n","    previous_text = \"\"\n","    for segment in segment_audio(audio_file, overlap=5):\n","        inputs = feature_extractor(segment, return_tensors=\"pt\", sampling_rate=16000)\n","        inputs = inputs.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model.generate(inputs.input_features)\n","\n","        current_text = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n","\n","        if previous_text:\n","            overlap_index = current_text.find(previous_text.split()[-1])\n","            if overlap_index != -1:\n","                current_text = current_text[overlap_index + len(previous_text.split()[-1]):].strip()\n","\n","        texts.append(current_text)\n","        previous_text = current_text\n","\n","    return \" \".join(texts)\n","\n","result = transcribe_segments(audio_file)\n","print(result)"]},{"cell_type":"code","source":[],"metadata":{"id":"JKsZDvUYNvG9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}