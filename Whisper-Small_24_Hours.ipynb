{"cells":[{"cell_type":"markdown","metadata":{"id":"LhXVcmgn-iYG"},"source":["**Install packages:** Install the necessary packages for running the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bOoRPAQ--iYI","outputId":"3e005a63-de97-4563-e205-5fe2450af2e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /private/var/folders/0c/j4c53d0102lcr2b5rk247yrm0000gn/T/pip-req-build-0nxo8x6h\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /private/var/folders/0c/j4c53d0102lcr2b5rk247yrm0000gn/T/pip-req-build-0nxo8x6h\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numba in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (0.59.0)\n","Requirement already satisfied: numpy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (1.26.4)\n","Requirement already satisfied: torch in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (2.2.1)\n","Requirement already satisfied: tqdm in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (4.65.0)\n","Requirement already satisfied: more-itertools in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (10.1.0)\n","Requirement already satisfied: tiktoken in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from openai-whisper==20231117) (0.6.0)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from numba->openai-whisper==20231117) (0.42.0)\n","Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from tiktoken->openai-whisper==20231117) (2023.10.3)\n","Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (4.9.0)\n","Requirement already satisfied: sympy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (1.12)\n","Requirement already satisfied: networkx in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch->openai-whisper==20231117) (2023.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"XfIZWRv0-iYJ","outputId":"b447c99e-0a98-400c-a3a3-bacee8fdcbfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (4.39.0.dev0)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (0.21.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (2023.10.3)\n","Requirement already satisfied: requests in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"GMjKPGme-iYJ","outputId":"c9a27d3f-2fa2-465d-e457-d8dcaa3fdb8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (2.2.1)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch) (2023.10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ec9k6Ave-iYJ","outputId":"725dca47-965c-427b-8c6b-cd3df9ab5e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (2.18.0)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.10.0)\n","Requirement already satisfied: aiohttp in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (0.21.4)\n","Requirement already satisfied: packaging in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install datasets"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"nCsklA3U-iYJ","outputId":"2c6a5074-dce1-4bb5-c524-23fd76d05bae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716208215809,"user_tz":-240,"elapsed":13806,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"]}],"source":["pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"paFimHno-iYJ","outputId":"fb5c4dc5-7686-4dc2-cec2-f94167148218"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: librosa in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (0.10.1)\n","Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.26.4)\n","Requirement already satisfied: scipy>=1.2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.2.0)\n","Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.59.0)\n","Requirement already satisfied: soundfile>=0.12.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.8.1)\n","Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (4.9.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from librosa) (1.0.3)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pooch>=1.0->librosa) (3.10.0)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pooch>=1.0->librosa) (23.1)\n","Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from pooch>=1.0->librosa) (2.31.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n","Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: pycparser in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install librosa"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"V1G-ErfU-iYK","outputId":"2c07ba45-511a-47f7-8f6a-abe81cbeced9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: jiwer in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (3.0.3)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jiwer) (8.1.7)\n","Requirement already satisfied: rapidfuzz<4,>=3 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jiwer) (3.6.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install jiwer"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"psrAo9rK-iYK","outputId":"f31a84e6-fd40-4105-8770-4c6ea40aae21"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (0.29.2)\n","Collecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: pyyaml in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (2.2.1)\n","Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (0.21.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n","Requirement already satisfied: requests in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/speech/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hInstalling collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.29.2\n","    Uninstalling accelerate-0.29.2:\n","      Successfully uninstalled accelerate-0.29.2\n","Successfully installed accelerate-0.30.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install accelerate -U"]},{"cell_type":"code","source":["pip install yt_dlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"wykL5YXB-jBW","executionInfo":{"status":"ok","timestamp":1716040422306,"user_tz":-240,"elapsed":32341,"user":{"displayName":"Tigran Gaplanyan","userId":"05332419263568644862"}},"outputId":"353a65e0-af6d-4110-8678-e6f62c69f42e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting yt_dlp\n","  Downloading yt_dlp-2024.4.9-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting brotli (from yt_dlp)\n","  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2024.2.2)\n","Collecting mutagen (from yt_dlp)\n","  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycryptodomex (from yt_dlp)\n","  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.31.0)\n","Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.0.7)\n","Collecting websockets>=12.0 (from yt_dlp)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt_dlp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt_dlp) (3.7)\n","Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt_dlp\n","Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt_dlp-2024.4.9\n"]}]},{"cell_type":"markdown","metadata":{"id":"sxeKJ9zQ-iYK"},"source":["**Setting up PyTorch Device:** Check if CUDA is available and set the device to GPU if possible; otherwise, fall back to CPU."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"TszM2YX2-iYK","outputId":"a5fe5b35-715a-434c-e327-672663b12f87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"M936mNpv-iYK"},"source":["**Dataset Preparation:** Import necessary libraries and define a function to create a dataset from a TSV file, which reads the file, adjusts the file paths, and converts the data into a Hugging Face dataset format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECaHxRSo-iYL"},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","import pandas as pd\n","from pathlib import Path\n","\n","base_path = Path('cv-corpus-16.1-2023-12-06/hy-AM')\n","\n","def create_dataset_from_dataframe(split_name, base_path):\n","    df = pd.read_csv(base_path / f'{split_name}.tsv', sep='\\t')\n","    clips_path = base_path / 'clips'\n","    df['path'] = df['path'].apply(lambda x: str(clips_path / x))\n","    dataset = Dataset.from_pandas(df)\n","    return dataset\n","\n","my_common_voice = DatasetDict()\n","\n","my_common_voice['train'] = create_dataset_from_dataframe('train', base_path)\n","my_common_voice['test'] = create_dataset_from_dataframe('test', base_path)\n","my_common_voice['validation'] = create_dataset_from_dataframe('dev', base_path)"]},{"cell_type":"markdown","metadata":{"id":"sl2MgB52-iYL"},"source":["**Clean Dataset:** Remove unnecessary columns like 'client_id', 'up_votes', etc., from the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L11M2swS-iYL"},"outputs":[],"source":["my_common_voice = my_common_voice.remove_columns([\"client_id\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accents\", \"variant\", \"locale\", \"segment\"])"]},{"cell_type":"markdown","metadata":{"id":"eH_pG0ce-iYL"},"source":["**Print Dataset Overview:** Display the structure of the dataset to confirm the changes and view the remaining columns."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"XR-JSgDL-iYL","outputId":"20e176ee-d80b-456d-d8bc-bab45f8c4284"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['path', 'sentence'],\n","        num_rows: 3794\n","    })\n","    test: Dataset({\n","        features: ['path', 'sentence'],\n","        num_rows: 2853\n","    })\n","    validation: Dataset({\n","        features: ['path', 'sentence'],\n","        num_rows: 2656\n","    })\n","})\n"]}],"source":["print(my_common_voice)"]},{"cell_type":"markdown","metadata":{"id":"44lBVz3K-iYL"},"source":["**Load Feature Extractor and Tokenizer:** Load the Whisper feature extractor and tokenizer from Hugging Face's Transformers library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvKQEiUz-iYM"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wb9eHJwA-iYM","outputId":"137037fc-0015-46be-c704-6bed9be9e2bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")"]},{"cell_type":"markdown","metadata":{"id":"Nk32NE5f-iYM"},"source":["**Transcription Tokenization and Decoding:** Perform tokenization on a sample text, and decode it to ensure the process retains the original text without any special tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9-z4Ap0-iYM","outputId":"bee2797a-3b45-4695-d839-c8313077ab88"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input:                 Բամբակենու ծայրատումը կատարվում է նախքան զանգվածային ծաղկումը։\n","Decoded w/ special:    <|startoftranscript|><|hy|><|transcribe|><|notimestamps|>Բամբակենու ծայրատումը կատարվում է նախքան զանգվածային ծաղկումը։<|endoftext|>\n","Decoded w/out special: Բամբակենու ծայրատումը կատարվում է նախքան զանգվածային ծաղկումը։\n","Are equal:             True\n"]}],"source":["input_str = my_common_voice[\"train\"][0][\"sentence\"]\n","labels = tokenizer(input_str).input_ids\n","decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n","decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n","\n","print(f\"Input:                 {input_str}\")\n","print(f\"Decoded w/ special:    {decoded_with_special}\")\n","print(f\"Decoded w/out special: {decoded_str}\")\n","print(f\"Are equal:             {input_str == decoded_str}\")"]},{"cell_type":"markdown","metadata":{"id":"F5KnPDyr-iYM"},"source":["**Print a Sample Entry:** Display a specific example from the training set to verify the data structure and contents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYjeD1U4-iYM","outputId":"2e304c8e-dc05-45cf-9360-a04f0b1df41b"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'path': 'cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39459169.mp3', 'sentence': 'Բամբակենու ծայրատումը կատարվում է նախքան զանգվածային ծաղկումը։'}\n"]}],"source":["print(my_common_voice[\"train\"][0])"]},{"cell_type":"markdown","metadata":{"id":"vIYkrSee-iYM"},"source":["**Define Data Preparation Function:** Create a function to process the audio data: loading, resampling, extracting features using Whisper, and encoding the labels for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DEEcYFb-iYM"},"outputs":[],"source":["import librosa\n","\n","def load_and_resample_audio(file_path, target_sr=16000):\n","    audio, _ = librosa.load(file_path, sr=target_sr)\n","    return {'array': audio, 'sampling_rate': target_sr}"]},{"cell_type":"markdown","metadata":{"id":"Xcj-A5wR-iYN"},"source":["**Apply Data Preparation:** Use the 'map' method to apply the data preparation function across the dataset, ensuring the data is ready for input into the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17LK7Vhu-iYN"},"outputs":[],"source":["def prepare_dataset(batch):\n","    path = batch[\"path\"]\n","    audio = load_and_resample_audio(path)\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"referenced_widgets":["c08b05ddad02406d90cb5cde9d3f4b03","def0d3b365f44eb4a0c61891fbacfdf6","d827d5d388034d5c91dc3b7b87e7d038"]},"id":"GCgu1xlx-iYN","outputId":"59bbf250-f9e1-4c68-89ca-d44d1cead9f3"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c08b05ddad02406d90cb5cde9d3f4b03","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"def0d3b365f44eb4a0c61891fbacfdf6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d827d5d388034d5c91dc3b7b87e7d038","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["my_common_voice = my_common_voice.map(prepare_dataset, remove_columns=my_common_voice.column_names[\"train\"])"]},{"cell_type":"markdown","metadata":{"id":"uGPClaGJ-iYN"},"source":["**Define Data Collator Class:** This class is designed to handle batching of speech sequences which might have variable lengths. It processes the input features and labels, padding them appropriately for the model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsb1ubjq-iYN"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","        return batch"]},{"cell_type":"markdown","metadata":{"id":"1uqyPsa6-iYN"},"source":["**Initialize Processor and Data Collator:** Load the Whisper processor and instantiate the custom data collator class using this processor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVj1i9oB-iYN","outputId":"46194855-a433-4944-8a2a-1dd585c934ee"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoPw6PFD-iYO"},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","metadata":{"id":"1JYcj5if-iYO"},"source":["**Setup Evaluation Metrics:** Import evaluation metrics specifically for word error rate (WER) and character error rate (CER) to be used during model testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFumQUr--iYO"},"outputs":[],"source":["import evaluate\n","\n","metric_wer = evaluate.load(\"wer\")\n","metric_cer = evaluate.load(\"cer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyt-7BXh-iYO"},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric_wer.compute(predictions=pred_str, references=label_str)\n","    cer = 100 * metric_cer.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer, \"cer\": cer}"]},{"cell_type":"markdown","metadata":{"id":"TCSXFBGb-iYP"},"source":["**Load Pretrained Model for Conditional Generation:** Import the WhisperForConditionalGeneration class from the transformers library and load a pretrained Whisper model configured for generating text in Armenian. The model configuration is also adjusted to set the generation language to Armenian."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jl8FmIqC-iYP"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n","model.generation_config.language = \"armenian\""]},{"cell_type":"markdown","metadata":{"id":"9bxLe30I-iYP"},"source":["**Configure Model Decoding:** Configure the model's decoding behavior by setting forced_decoder_ids to None and clearing any suppress_tokens. These settings adjust how the model generates output, ensuring that it does not force any specific decoder token IDs and does not suppress any tokens during the decoding phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXewpC55-iYP"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"markdown","metadata":{"id":"uAcTco-j-iYQ"},"source":["**Create Logging Directory** Checks if a logging directory exists and if not, it creates one. This directory will be used to store training logs, which are crucial for monitoring the training process and evaluating model performance over time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoKVZZkU-iYQ"},"outputs":[],"source":["import os\n","\n","path = \"logs/small_25_epoch_nohup_24_hours\"\n","if not os.path.exists(path):\n","    os.makedirs(path)\n","    logdir = path\n","else:\n","    logdir = path"]},{"cell_type":"markdown","metadata":{"id":"z2q39Oaz-iYQ"},"source":["**Define Training Arguments:** Setup various training parameters such as batch sizes, learning rates, and evaluation strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17ah8LK0-iYQ"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"whisper-small-hy-nohup-24-hours\",\n","    per_device_train_batch_size=16,\n","    logging_dir=logdir,\n","    gradient_accumulation_steps=1,\n","    learning_rate=1e-5,\n","    warmup_steps=1,\n","    num_train_epochs=1,\n","    gradient_checkpointing=True,\n","    fp16=False,\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_strategy = \"epoch\",\n","    evaluation_strategy = \"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"zjG9n-x2-iYR"},"source":["**Prepare Model for Training:** Move the model to the designated computing device (GPU or CPU)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"CpJL2uJq-iYR","outputId":"048a58e4-b132-4c22-f131-22e8c04e16e5"},"outputs":[{"data":{"text/plain":["WhisperForConditionalGeneration(\n","  (model): WhisperModel(\n","    (encoder): WhisperEncoder(\n","      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n","      (embed_positions): Embedding(1500, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperEncoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): WhisperDecoder(\n","      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n","      (embed_positions): WhisperPositionalEmbedding(448, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperDecoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"TOBUl_cx-iYR"},"source":["**Initialize Training Setup:** Combine all necessary components like the model, datasets, data collator, and metrics function into the Seq2SeqTrainer for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPqDj8y4-iYR"},"outputs":[],"source":["from transformers import TrainerCallback\n","\n","class SaveLastModelCallback(TrainerCallback):\n","    def on_train_end(self, args, state, control, **kwargs):\n","        trainer.save_model(output_dir=args.output_dir + \"/last_epoch\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIgPxSgz-iYR"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=my_common_voice[\"train\"],\n","    eval_dataset=my_common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n","    callbacks=[SaveLastModelCallback()],\n",")"]},{"cell_type":"markdown","metadata":{"id":"8XCp1AXC-iYR"},"source":["**Training:** Execute the training process using the trainer configuration established in previous cells."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_uXWB9wW-iYR","outputId":"38812962-b9da-42f1-ae14-fa9b07aa9cab"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/user1801/anaconda3/envs/speech/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:11, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","      <th>Cer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0.360475</td>\n","      <td>41.666667</td>\n","      <td>11.032028</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n","There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"]},{"data":{"text/plain":["TrainOutput(global_step=1, training_loss=4.760254887514748e-05, metrics={'train_runtime': 18.7099, 'train_samples_per_second': 0.534, 'train_steps_per_second': 0.053, 'total_flos': 2885854003200000.0, 'train_loss': 4.760254887514748e-05, 'epoch': 1.0})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGWmu8Ck-iYR","outputId":"e74359d7-4d2f-4b0b-bf11-e142633ff372"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["WhisperForConditionalGeneration(\n","  (model): WhisperModel(\n","    (encoder): WhisperEncoder(\n","      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n","      (embed_positions): Embedding(1500, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperEncoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): WhisperDecoder(\n","      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n","      (embed_positions): WhisperPositionalEmbedding(448, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperDecoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor\n","import evaluate\n","import librosa\n","\n","metric_wer = evaluate.load(\"wer\")\n","metric_cer = evaluate.load(\"cer\")\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n","model.generation_config.language = \"armenian\"\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["9480825e31a948e6b4d391e123a36be6"]},"id":"HhGqyWgq-iYS","outputId":"9c21a0af-e02a-4a09-d984-80308165c693"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9480825e31a948e6b4d391e123a36be6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def map_to_pred(batch):\n","    path = batch[\"path\"]\n","    audio, _ = librosa.load(batch[\"path\"], sr=16000)\n","    input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n","    batch[\"reference\"] = processor.tokenizer._normalize(batch['sentence'])\n","\n","    with torch.no_grad():\n","        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n","    transcription = processor.decode(predicted_ids)\n","    batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n","    return batch\n","\n","result = my_common_voice[\"test\"].map(map_to_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D22AwGaJ-iYS","outputId":"0cfd6eab-ddd1-4af7-fd9f-2eb21598f391"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test WER: 200.000\n","Test CER: 136.511\n"]}],"source":["print(\"Test WER: {:.3f}\".format(100 * metric_wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"])))\n","print(\"Test CER: {:.3f}\".format(100 * metric_cer.compute(references=result[\"reference\"], predictions=result[\"prediction\"])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8llj7f8p-iYS"},"outputs":[],"source":["model.eval()\n","\n","def transcribe_audio(audio_file):\n","    audio, sampling_rate = librosa.load(audio_file, sr=16000, mono=True)\n","    inputs = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=sampling_rate)\n","    inputs = inputs.to(device)\n","\n","    with torch.no_grad():\n","        predictions = model.generate(inputs.input_features)\n","\n","    text = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6j5Y0xQ2-iYS","outputId":"07c75d9f-b0b5-4479-8e67-7ac5d6e14ccd"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Մատմողն աշխատանք է ստանում գրադարանում\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39509489.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YTN9nux-iYS","outputId":"c6b7ed07-2433-4adf-8147-62c860d01cd9"},"outputs":[{"name":"stdout","output_type":"stream","text":[" այս պայջարով, որոշ ծրագրայի բագեր չյն հայդնավ էրվում միջև ծրագրիր չիտ էրվում.\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39517769.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6NfZAc6-iYS","outputId":"06e9cea4-3187-47c5-c340-21838aa412de"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Նակոչ չերանութը պիտակամորդների դեմ բայկարել, մարկարեներ է չեին խոսումներ անունից.\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39427421.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"balh5B2H-iYS","outputId":"04cdd3e8-48f6-434b-fea3-6235ccda7cd4"},"outputs":[{"name":"stdout","output_type":"stream","text":[" դեկլենց կսետցվալ ձամակաջքերով, պեծ այմբիցի աղեկտուր հոսքերով, որնույնի սկոր.\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39295677.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TueFJf5D-iYT","outputId":"811d4490-1780-40e4-8f8f-eec14a18b15d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[youtube] Extracting URL: https://www.youtube.com/watch?v=-OhScYMGqvc\n","[youtube] -OhScYMGqvc: Downloading webpage\n","[youtube] -OhScYMGqvc: Downloading ios player API JSON\n","[youtube] -OhScYMGqvc: Downloading android player API JSON\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"-OhScYMGqvc\")\n"]},{"name":"stdout","output_type":"stream","text":["[youtube] -OhScYMGqvc: Downloading m3u8 information\n","[info] -OhScYMGqvc: Downloading 1 format(s): 251\n","[download] Destination: -OhScYMGqvc.webm\n","[download] 100% of    3.10MiB in 00:00:01 at 2.98MiB/s   \n","[ExtractAudio] Destination: -OhScYMGqvc.mp3\n","Deleting original file -OhScYMGqvc.webm (pass -k to keep)\n"," Հառստակամ բանյգանչում խայլածի հոմոտլեր, չաց չիպիտի, սգացել էք վերչերսուն չկանիգ մոզում Հառստակամ բանյգանչուն մասին. բոխչույն, ես ուն արեն սգացել է, քե որը շել ենք ստախսել հաղով տաշատ, որ ինտածկում գոսյնք Հառստակամ բանյգանչուն վայս տանուն երագրաղոթյան մասին. Հառստակամ բանյգանչուն ես հատեր պատկեր այստում են այսպես. բեծնավ, վամ են դեպկում ինչ առեստակամ բանականություն, մի դոպը. Նես հածքելին է, որ արեստակամ բանականություն եմ վեր այսնալոյ, բալորմասնակիտություն ները կամ որ մատկանս կալի կայլ է ոսչինին էլո.  դեղ արստական բանականսուն նայսոր ամենուրը. բրջսկության մեջ բարդ վիր հատություն ներան աղտներ են ստահսվում. արստական բանական չամստահսվածնե կարնե դության անդեսներում են են կայցվում. արվոտներ ալ կան որ հախտում են աշխալ հարչակ շախմատ իսներին. շախմատ. արստական բանական ստահսլի սմատականսմո դարաջին են գալի ստահջ թին. դամեն եմ եգ տալի արաճ ամերիկեն էողղնե եյդ էխտալոգ եկան են այսկորթի կրկչե  Հայդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ այդ  Նրանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ եմ այս հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հանձտ հա�  Հանտ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այդ մանի այ ստ միր ենք ուն անտավ եմ ավահիր ադարձիչների մասին տավակություն ներ ու ինչ հասում ապներ եմ ասին ասին ապներ ասում ավներ ասում ավներ ավներ ավներ ասին ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ ավներ  Մարհան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հանվան հան\n"]}],"source":["import yt_dlp\n","import os\n","\n","\n","def download_youtube_audio(url):\n","    ydl_opts = {\n","        'format': 'bestaudio/best',\n","        'postprocessors': [{\n","            'key': 'FFmpegExtractAudio',\n","            'preferredcodec': 'mp3',\n","            'preferredquality': '192',\n","        }],\n","        'outtmpl': '%(id)s.%(ext)s',\n","        'quiet': False\n","    }\n","    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n","        info_dict = ydl.extract_info(url, download=True)\n","        filename = ydl.prepare_filename(info_dict)\n","        base, ext = os.path.splitext(filename)\n","        return base + '.mp3'\n","\n","audio_file = download_youtube_audio(\"https://www.youtube.com/watch?v=-OhScYMGqvc\")\n","\n","def segment_audio(audio_file, segment_length=30, overlap=5, sample_rate=16000):\n","    audio, sr = librosa.load(audio_file, sr=sample_rate, mono=True)\n","    total_length = librosa.get_duration(y=audio, sr=sr)\n","\n","    start = 0\n","    while start + segment_length <= total_length:\n","        end = start + segment_length\n","        yield audio[int(start * sr):int(end * sr)]\n","        start += (segment_length - overlap)\n","\n","def transcribe_segments(audio_file):\n","    texts = []\n","    previous_text = \"\"\n","    for segment in segment_audio(audio_file, overlap=5):\n","        inputs = feature_extractor(segment, return_tensors=\"pt\", sampling_rate=16000)\n","        inputs = inputs.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model.generate(inputs.input_features)\n","\n","        current_text = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n","\n","        if previous_text:\n","            overlap_index = current_text.find(previous_text.split()[-1])\n","            if overlap_index != -1:\n","                current_text = current_text[overlap_index + len(previous_text.split()[-1]):].strip()\n","\n","        texts.append(current_text)\n","        previous_text = current_text\n","\n","    return \" \".join(texts)\n","\n","result = transcribe_segments(audio_file)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"dddZxlB2-iYT"},"source":["**Load Pretrained Model:** Load a Whisper model pre-trained for conditional generation with specific configurations set for Armenian language."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZK7AnoA-iYT","outputId":"7bbcc2fe-9c1c-4bbd-d981-0db94fd177ca"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["WhisperForConditionalGeneration(\n","  (model): WhisperModel(\n","    (encoder): WhisperEncoder(\n","      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n","      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n","      (embed_positions): Embedding(1500, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperEncoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): WhisperDecoder(\n","      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n","      (embed_positions): WhisperPositionalEmbedding(448, 768)\n","      (layers): ModuleList(\n","        (0-11): 12 x WhisperDecoderLayer(\n","          (self_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): WhisperSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",")"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor\n","import evaluate\n","import librosa\n","\n","metric_wer = evaluate.load(\"wer\")\n","metric_cer = evaluate.load(\"cer\")\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"whisper-small-hy-nohup-24-hours/checkpoint-5950\")\n","model.generation_config.language = \"armenian\"\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"whisper-small-hy-nohup-24-hours/checkpoint-5950\")\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Armenian\", task=\"transcribe\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["4ddd88b247074ed991b43121283d8983"]},"id":"IvCJEfw3-iYT","outputId":"7f37fb0b-a6ac-4e5c-bc7c-529eab5d3e6c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ddd88b247074ed991b43121283d8983","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def map_to_pred(batch):\n","    path = batch[\"path\"]\n","    audio, _ = librosa.load(batch[\"path\"], sr=16000)\n","    input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n","    batch[\"reference\"] = processor.tokenizer._normalize(batch['sentence'])\n","\n","    with torch.no_grad():\n","        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n","    transcription = processor.decode(predicted_ids)\n","    batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n","    return batch\n","\n","result = my_common_voice[\"test\"].map(map_to_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pqz21jQF-iYT","outputId":"98f2d4ed-0025-4f1e-8ba6-f5e5f556f68f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test WER: 40.278\n","Test CER: 10.432\n"]}],"source":["print(\"Test WER: {:.3f}\".format(100 * metric_wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"])))\n","print(\"Test CER: {:.3f}\".format(100 * metric_cer.compute(references=result[\"reference\"], predictions=result[\"prediction\"])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDGdVuoC-iYT"},"outputs":[],"source":["model.eval()\n","\n","def transcribe_audio(audio_file):\n","    audio, sampling_rate = librosa.load(audio_file, sr=16000, mono=True)\n","    inputs = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=sampling_rate)\n","    inputs = inputs.to(device)\n","\n","    with torch.no_grad():\n","        predictions = model.generate(inputs.input_features)\n","\n","    text = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmB5OoxM-iYT","outputId":"6f1ef132-23a3-47a6-da6a-e0ad8df79e34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Պատմողն աշխատանք է ստանում գրադարանում։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39509489.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOPCJbDz-iYT","outputId":"67aff062-3b96-4816-badb-262e0bc2921b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Այս պատճառով, որոշ ծրագրային բագեր չեն հայտնաբերվում մինչև ծրագիրը չի տեստավորվում։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39517769.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFVkoMTq-iYU","outputId":"e9702c05-ab24-40a8-a954-a1c557916bbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Նակոչ չէր անում սպիտակամորդների դեմ պայքարել՝ մարգարեները չէին խոսումները անունից։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39427421.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWbwh5tf-iYU","outputId":"23633787-c504-49f1-8672-4d70e2852c58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Թեկլեն սկսետ զգալ ցամա կաչքերով, բայց այնպիսի աղեկտուռ խոսքերով, ուրնունի սկոր։\n"]}],"source":["result = transcribe_audio('cv-corpus-16.1-2023-12-06/hy-AM/clips/common_voice_hy-AM_39295677.mp3')\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1y9imox-iYU","outputId":"95a33faf-47f4-42dd-8be9-53eb6b21eaad"},"outputs":[{"name":"stdout","output_type":"stream","text":["[youtube] Extracting URL: https://www.youtube.com/watch?v=-OhScYMGqvc\n","[youtube] -OhScYMGqvc: Downloading webpage\n","[youtube] -OhScYMGqvc: Downloading ios player API JSON\n","[youtube] -OhScYMGqvc: Downloading android player API JSON\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: [youtube] Skipping player responses from android clients (got player responses for video \"aQvGIIdgFDM\" instead of \"-OhScYMGqvc\")\n"]},{"name":"stdout","output_type":"stream","text":["[youtube] -OhScYMGqvc: Downloading m3u8 information\n","[info] -OhScYMGqvc: Downloading 1 format(s): 251\n","[download] Destination: -OhScYMGqvc.webm\n","[download] 100% of    3.10MiB in 00:00:00 at 5.40MiB/s   \n","[ExtractAudio] Destination: -OhScYMGqvc.mp3\n","Deleting original file -OhScYMGqvc.webm (pass -k to keep)\n","Արստական բանականձին, խայլացի ռոբոտներ, չա ճիփիթի։ զգացել եք վերջերից միջքան իմպոզում արստական բանականության մասին։ երսունարենը զգացելենք օրոշել ենք ստեղծել հաղուրդաշար, որի ընթացքում կոգոսենք արստական բանականության, վայեստանում նրա գրաղության մասին։Արստական բանականությունը շատերը պատկերաստումը նայսպես… Պեցնավ, ամեն դեպքում ինչը արեստական բանականությունը, մինուբը… լեսածկլիներ, որ արեստական բանականությունը վերասնալրե՝ բելորմասնագիտությունները, կամ որ մարմարդկանց կայլևս չինինելու… Դարեստական բանականությունները, կամ արհտկանց կառիկ այլևշ չիրինելու։ դե՛արհստական բանականությունն այսօրը ամենուր է, վժշկության մեջ բիրահատություններանով ռոբոտներ են ստեացվում, արհստական բանականությամբ ստեացվածնէկարները ցուցանանդեսներում են ներկայացվում, ռոբոտներել կան, որհաղթում են աշխարահաարչակ շախմատիսներին…շախմատ…Իսգիմարհրհհահրհական բանականություն ամարհականություն առաչինը Ոմերիկյան օգնէիայի տեղտալոգեկան ինկերության այս գործիքը չէրի ներ հայտնի։ շատ կարծ ժամանակում չատ ջիփիթին այնպիսի հաջողություն գրանցեց, որ նրա մասին ոսում էին ամենուրը…Չատգոտ, նարդյն շափաթական ավերիքան փառիզմիլյուն ակտիվ օգտատերումի…լՕվտսնյուն ոկտիռերով… Ինչպես կարող է մոկնել ձեզ այսոր…Սա առաչին հարձնը, որը հայթնում է էքրանիտ, և փրծումէ շպվվել խելացի բոթի հետ, դ եմքով նսլռոմ էր սաղորդումայունի… Օգնեայա շարունակն ավելէսներ չա ջփիքի հիմքում եղած տվյալները, օրինակիտեսն ավորդարի ետ ինչվեր հարց էի տարիս, հիմնականում պատասխանում էր գորիր տվյալները երկոզոքսանեք թվականի սեպտենդերի են ունինքը չի կարող պատասխանե՞՝ բայց ջփիփիչորդուրպուտարբերեքը արդեն երկոզոքսանեքթվականի ապտիրի տվյալներունի՝ հաղոէ՞էդ պաճառով շատելնասում են, որ գուգգլի բարգնոյինրուն մածկի գրոքը շատավելի լավ ելու ուղղությամբ… իսկ մչևնելանք աշխատումայն՝ գուգլները, շափանիշներով գերազանձումը չատ Գիպիթյի մոդելներին… աաաաա�ocօչոր մրցավասկրինի… հհը՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞՞ - Թել, գիլտում աչինքա։\n"]}],"source":["import yt_dlp\n","import os\n","\n","\n","def download_youtube_audio(url):\n","    ydl_opts = {\n","        'format': 'bestaudio/best',\n","        'postprocessors': [{\n","            'key': 'FFmpegExtractAudio',\n","            'preferredcodec': 'mp3',\n","            'preferredquality': '192',\n","        }],\n","        'outtmpl': '%(id)s.%(ext)s',\n","        'quiet': False\n","    }\n","    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n","        info_dict = ydl.extract_info(url, download=True)\n","        filename = ydl.prepare_filename(info_dict)\n","        base, ext = os.path.splitext(filename)\n","        return base + '.mp3'\n","\n","audio_file = download_youtube_audio(\"https://www.youtube.com/watch?v=-OhScYMGqvc\")\n","\n","def segment_audio(audio_file, segment_length=30, overlap=5, sample_rate=16000):\n","    audio, sr = librosa.load(audio_file, sr=sample_rate, mono=True)\n","    total_length = librosa.get_duration(y=audio, sr=sr)\n","\n","    start = 0\n","    while start + segment_length <= total_length:\n","        end = start + segment_length\n","        yield audio[int(start * sr):int(end * sr)]\n","        start += (segment_length - overlap)\n","\n","def transcribe_segments(audio_file):\n","    texts = []\n","    previous_text = \"\"\n","    for segment in segment_audio(audio_file, overlap=5):\n","        inputs = feature_extractor(segment, return_tensors=\"pt\", sampling_rate=16000)\n","        inputs = inputs.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model.generate(inputs.input_features)\n","\n","        current_text = tokenizer.batch_decode(predictions, skip_special_tokens=True)[0]\n","\n","        if previous_text:\n","            overlap_index = current_text.find(previous_text.split()[-1])\n","            if overlap_index != -1:\n","                current_text = current_text[overlap_index + len(previous_text.split()[-1]):].strip()\n","\n","        texts.append(current_text)\n","        previous_text = current_text\n","\n","    return \" \".join(texts)\n","\n","result = transcribe_segments(audio_file)\n","print(result)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}